{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ResNet50 for CIFAR-100","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOp1PTkYSzSj2nDLmB99b5g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2002a1cc20c34b92ac12018f08fbe85e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_67cd7b6bcf964baa8fa468d6b64bb4bb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a81f4e12209544c1a9ade85601427d64","IPY_MODEL_3bf7f6ce75ff4ea8b2f2fd70c4a3eda2"]}},"67cd7b6bcf964baa8fa468d6b64bb4bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a81f4e12209544c1a9ade85601427d64":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_17330e3dd11a4218884773b50bedef4d","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fa8d349e17e74df79f783ebdd4defeec"}},"3bf7f6ce75ff4ea8b2f2fd70c4a3eda2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6d4d2793ad474086bd8f1d88020ea8e0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"169009152it [00:30, 16300740.66it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cf63d9e2c97443a5a15c81134b1bd9e7"}},"17330e3dd11a4218884773b50bedef4d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fa8d349e17e74df79f783ebdd4defeec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6d4d2793ad474086bd8f1d88020ea8e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cf63d9e2c97443a5a15c81134b1bd9e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"motA2RSlKan-","colab_type":"code","colab":{}},"source":["\"\"\"resnet in pytorch\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n","    Deep Residual Learning for Image Recognition\n","    https://arxiv.org/abs/1512.03385v1\n","    code frome here: \n","    https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/resnet.py\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","\n","class BasicBlock(nn.Module):\n","    \"\"\"Basic Block for resnet 18 and resnet 34\n","    \"\"\"\n","\n","    #BasicBlock and BottleNeck block \n","    #have different output size\n","    #we use class attribute expansion\n","    #to distinct\n","    expansion = 1\n","\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super().__init__()\n","\n","        #residual function\n","        self.residual_function = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n","        )\n","\n","        #shortcut\n","        self.shortcut = nn.Sequential()\n","\n","        #the shortcut output dimension is not the same with residual function\n","        #use 1*1 convolution to match the dimension\n","        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n","            )\n","        \n","    def forward(self, x):\n","        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n","\n","class BottleNeck(nn.Module):\n","    \"\"\"Residual block for resnet over 50 layers\n","    \"\"\"\n","    expansion = 4\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super().__init__()\n","        self.residual_function = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n","            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n","        )\n","\n","        self.shortcut = nn.Sequential()\n","\n","        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n","                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n","            )\n","        \n","    def forward(self, x):\n","        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n","    \n","class ResNet(nn.Module):\n","\n","    def __init__(self, block, num_block, num_classes=100):\n","        super().__init__()\n","\n","        self.in_channels = 64\n","\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True))\n","        #we use a different inputsize than the original paper\n","        #so conv2_x's stride is 1\n","        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n","        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n","        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n","        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n","        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","    def _make_layer(self, block, out_channels, num_blocks, stride):\n","        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the \n","        same as a neuron netowork layer, ex. conv layer), one layer may \n","        contain more than one residual block \n","        Args:\n","            block: block type, basic block or bottle neck block\n","            out_channels: output depth channel number of this layer\n","            num_blocks: how many blocks per layer\n","            stride: the stride of the first block of this layer\n","        \n","        Return:\n","            return a resnet layer\n","        \"\"\"\n","\n","        # we have num_block blocks per layer, the first block \n","        # could be 1 or 2, other blocks would always be 1\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_channels, out_channels, stride))\n","            self.in_channels = out_channels * block.expansion\n","        \n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        output = self.conv1(x)\n","        output = self.conv2_x(output)\n","        output = self.conv3_x(output)\n","        output = self.conv4_x(output)\n","        output = self.conv5_x(output)\n","        output = self.avg_pool(output)\n","        output = output.view(output.size(0), -1)\n","        output = self.fc(output)\n","\n","        return output \n","\n","def resnet18():\n","    \"\"\" return a ResNet 18 object\n","    \"\"\"\n","    return ResNet(BasicBlock, [2, 2, 2, 2])\n","\n","def resnet34():\n","    \"\"\" return a ResNet 34 object\n","    \"\"\"\n","    return ResNet(BasicBlock, [3, 4, 6, 3])\n","\n","def resnet50():\n","    \"\"\" return a ResNet 50 object\n","    \"\"\"\n","    return ResNet(BottleNeck, [3, 4, 6, 3])\n","\n","def resnet101():\n","    \"\"\" return a ResNet 101 object\n","    \"\"\"\n","    return ResNet(BottleNeck, [3, 4, 23, 3])\n","\n","def resnet152():\n","    \"\"\" return a ResNet 152 object\n","    \"\"\"\n","    return ResNet(BottleNeck, [3, 8, 36, 3])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3lKFMi0MAt6Q","colab_type":"code","outputId":"0e6680d8-bd7a-4cb5-ce55-859fc9313a6d","executionInfo":{"status":"ok","timestamp":1583342757612,"user_tz":-420,"elapsed":2761,"user":{"displayName":"Mike Menshikov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMUQ2AiYyDfiizTUNCS5AR7vleZBvoaW81Hw3jfw=s64","userId":"07903239830916095219"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%matplotlib inline\n","\n","import torch\n","import torch.nn as nn\n","from torchvision import datasets, transforms\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","    \n","print('Using PyTorch version:', torch.__version__, ' Device:', device)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using PyTorch version: 1.4.0  Device: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qNMNHsHIChMj","colab_type":"code","outputId":"178e9303-dc90-4f07-ec83-45d8a904f173","executionInfo":{"status":"ok","timestamp":1583342783071,"user_tz":-420,"elapsed":11433,"user":{"displayName":"Mike Menshikov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMUQ2AiYyDfiizTUNCS5AR7vleZBvoaW81Hw3jfw=s64","userId":"07903239830916095219"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model = resnet50().to(device)\n","print(model)\n","\n","# Hyper-parameters\n","num_epochs = 150\n","learning_rate = 0.11\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), \n","                            lr=learning_rate,\n","                            weight_decay=5e-4,\n","                            momentum=0.5)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["ResNet(\n","  (conv1): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","  )\n","  (conv2_x): Sequential(\n","    (0): BottleNeck(\n","      (residual_function): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BottleNeck(\n","      (residual_function): Sequential(\n","        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (shortcut): Sequential()\n","    )\n","    (2): BottleNeck(\n","      (residual_function): Sequential(\n","        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (conv3_x): Sequential(\n","    (0): BottleNeck(\n","      (residual_function): Sequential(\n","        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","        (6): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BottleNeck(\n","      (residual_function): Sequential(\n","        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","        (6): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (shortcut): Sequential()\n","    )\n","    (2): BottleNeck(\n","      (residual_function): Sequential(\n","        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","        (6): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (shortcut): Sequential()\n","    )\n","    (3): BottleNeck(\n","      (residual_function): Sequential(\n","        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","        (6): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (conv4_x): Sequential(\n","    (0): BottleNeck(\n","      (residual_function): Sequential(\n","        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","        (6): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (shortcut): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BottleNeck(\n","      (residual_function): Sequential(\n","        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","        (6): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (shortcut): Sequential()\n","    )\n","    (2): BottleNeck(\n","      (residual_function): Sequential(\n","        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","        (6): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (shortcut): Sequential()\n","    )\n","    (3): BottleNeck(\n","      (residual_function): Sequential(\n","        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","        (6): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (shortcut): Sequential()\n","    )\n","    (4): BottleNeck(\n","      (residual_function): Sequential(\n","        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","        (6): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (shortcut): Sequential()\n","    )\n","    (5): BottleNeck(\n","      (residual_function): Sequential(\n","        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","        (6): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (conv5_x): Sequential(\n","    (0): BottleNeck(\n","      (residual_function): Sequential(\n","        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","        (6): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (shortcut): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BottleNeck(\n","      (residual_function): Sequential(\n","        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","        (6): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (shortcut): Sequential()\n","    )\n","    (2): BottleNeck(\n","      (residual_function): Sequential(\n","        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU(inplace=True)\n","        (6): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=2048, out_features=100, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N_zmSZHhAupB","colab_type":"code","outputId":"fec1d890-e6d5-4a6e-c16d-d24e72d06a38","executionInfo":{"status":"ok","timestamp":1583342804938,"user_tz":-420,"elapsed":17040,"user":{"displayName":"Mike Menshikov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMUQ2AiYyDfiizTUNCS5AR7vleZBvoaW81Hw3jfw=s64","userId":"07903239830916095219"}},"colab":{"base_uri":"https://localhost:8080/","height":99,"referenced_widgets":["2002a1cc20c34b92ac12018f08fbe85e","67cd7b6bcf964baa8fa468d6b64bb4bb","a81f4e12209544c1a9ade85601427d64","3bf7f6ce75ff4ea8b2f2fd70c4a3eda2","17330e3dd11a4218884773b50bedef4d","fa8d349e17e74df79f783ebdd4defeec","6d4d2793ad474086bd8f1d88020ea8e0","cf63d9e2c97443a5a15c81134b1bd9e7"]}},"source":["# Normalize training set together with augmentation\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(30),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762))])\n","\n","# Normalize test set same as training set without augmentation\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762))])\n","\n","batch_size = 100\n","\n","train_dataset = datasets.CIFAR100('./data', \n","                               train=True, \n","                               download=True, \n","                               transform=transform_train)\n","\n","test_dataset = datasets.CIFAR100('./data', \n","                                    train=False, \n","                                    transform=transform_test)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n","                                                batch_size=batch_size, \n","                                                shuffle=False)\n","print('Done!')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2002a1cc20c34b92ac12018f08fbe85e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/cifar-100-python.tar.gz to ./data\n","Done!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bFwK_eQTl4td","colab_type":"code","colab":{}},"source":["# For updating learning rate\n","def update_lr(optimizer, lr):    \n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","    print(f'Learning rate updated. New learning rate = {lr}\\n')\n","\n","\n","def train(epoch, log_interval=100):\n","    # Set model to training mode\n","    model.train()\n","    \n","    # Loop over each batch from the training set\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        # Copy data to GPU if needed\n","        data = data.to(device)\n","        target = target.to(device)\n","\n","        # Zero gradient buffers\n","        optimizer.zero_grad() \n","        \n","        # Pass data through the network\n","        output = model(data)\n","\n","        # Calculate loss\n","        loss = criterion(output, target)\n","\n","        # Backpropagate\n","        loss.backward()\n","        \n","        # Update weights\n","        optimizer.step()\n","        \n","        if batch_idx % log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.data.item()))\n","            \n","def validate(loss_vector, accuracy_vector):\n","    model.eval()\n","    val_loss, correct = 0, 0\n","    for data, target in test_loader:\n","        data = data.to(device)\n","        target = target.to(device)\n","        output = model(data)\n","        val_loss += criterion(output, target).data.item()\n","        pred = output.data.max(1)[1] # get the index of the max log-probability\n","        correct += pred.eq(target.data).cpu().sum()\n","\n","    val_loss /= len(test_loader)\n","    loss_vector.append(val_loss)\n","\n","    accuracy = 100. * correct.to(torch.float32) / len(test_loader.dataset)\n","    accuracy_vector.append(accuracy)\n","    \n","    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        val_loss, correct, len(test_loader.dataset), accuracy))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4OBMpCC4mIWB","colab_type":"code","outputId":"857617ee-2588-47e9-f9dd-dc1f106c2e33","executionInfo":{"status":"ok","timestamp":1583357142052,"user_tz":-420,"elapsed":705262,"user":{"displayName":"Mike Menshikov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMUQ2AiYyDfiizTUNCS5AR7vleZBvoaW81Hw3jfw=s64","userId":"07903239830916095219"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["%%time\n","curr_lr = learning_rate\n","lossv, accv = [], []\n","for epoch in range(1, num_epochs + 1):\n","    train(epoch)\n","    validate(lossv, accv)\n","\n","     # Decay learning rate\n","    if ((epoch+1) % 20) == 0 & (curr_lr > 2e-5):\n","        curr_lr /= 2\n","        update_lr(optimizer, curr_lr)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Train Epoch: 1 [0/50000 (0%)]\tLoss: 4.671367\n","Train Epoch: 1 [10000/50000 (20%)]\tLoss: 4.843065\n","Train Epoch: 1 [20000/50000 (40%)]\tLoss: 4.444541\n","Train Epoch: 1 [30000/50000 (60%)]\tLoss: 4.108718\n","Train Epoch: 1 [40000/50000 (80%)]\tLoss: 4.058879\n","\n","Validation set: Average loss: 3.8583, Accuracy: 891/10000 (9%)\n","\n","Train Epoch: 2 [0/50000 (0%)]\tLoss: 3.975603\n","Train Epoch: 2 [10000/50000 (20%)]\tLoss: 3.795005\n","Train Epoch: 2 [20000/50000 (40%)]\tLoss: 3.973510\n","Train Epoch: 2 [30000/50000 (60%)]\tLoss: 3.456720\n","Train Epoch: 2 [40000/50000 (80%)]\tLoss: 3.416188\n","\n","Validation set: Average loss: 3.4352, Accuracy: 1614/10000 (16%)\n","\n","Train Epoch: 3 [0/50000 (0%)]\tLoss: 3.450189\n","Train Epoch: 3 [10000/50000 (20%)]\tLoss: 3.609984\n","Train Epoch: 3 [20000/50000 (40%)]\tLoss: 3.443965\n","Train Epoch: 3 [30000/50000 (60%)]\tLoss: 2.983057\n","Train Epoch: 3 [40000/50000 (80%)]\tLoss: 3.272163\n","\n","Validation set: Average loss: 3.1139, Accuracy: 2242/10000 (22%)\n","\n","Train Epoch: 4 [0/50000 (0%)]\tLoss: 3.059500\n","Train Epoch: 4 [10000/50000 (20%)]\tLoss: 3.113586\n","Train Epoch: 4 [20000/50000 (40%)]\tLoss: 2.812072\n","Train Epoch: 4 [30000/50000 (60%)]\tLoss: 3.002837\n","Train Epoch: 4 [40000/50000 (80%)]\tLoss: 3.025979\n","\n","Validation set: Average loss: 2.9195, Accuracy: 2668/10000 (27%)\n","\n","Train Epoch: 5 [0/50000 (0%)]\tLoss: 2.997527\n","Train Epoch: 5 [10000/50000 (20%)]\tLoss: 3.023171\n","Train Epoch: 5 [20000/50000 (40%)]\tLoss: 2.895472\n","Train Epoch: 5 [30000/50000 (60%)]\tLoss: 2.626050\n","Train Epoch: 5 [40000/50000 (80%)]\tLoss: 2.743484\n","\n","Validation set: Average loss: 2.7264, Accuracy: 3146/10000 (31%)\n","\n","Train Epoch: 6 [0/50000 (0%)]\tLoss: 2.743694\n","Train Epoch: 6 [10000/50000 (20%)]\tLoss: 2.777755\n","Train Epoch: 6 [20000/50000 (40%)]\tLoss: 2.703563\n","Train Epoch: 6 [30000/50000 (60%)]\tLoss: 2.583579\n","Train Epoch: 6 [40000/50000 (80%)]\tLoss: 2.809141\n","\n","Validation set: Average loss: 2.5233, Accuracy: 3602/10000 (36%)\n","\n","Train Epoch: 7 [0/50000 (0%)]\tLoss: 2.467645\n","Train Epoch: 7 [10000/50000 (20%)]\tLoss: 2.186107\n","Train Epoch: 7 [20000/50000 (40%)]\tLoss: 2.549268\n","Train Epoch: 7 [30000/50000 (60%)]\tLoss: 2.511773\n","Train Epoch: 7 [40000/50000 (80%)]\tLoss: 2.576196\n","\n","Validation set: Average loss: 2.4192, Accuracy: 3741/10000 (37%)\n","\n","Train Epoch: 8 [0/50000 (0%)]\tLoss: 2.054735\n","Train Epoch: 8 [10000/50000 (20%)]\tLoss: 2.279623\n","Train Epoch: 8 [20000/50000 (40%)]\tLoss: 2.419810\n","Train Epoch: 8 [30000/50000 (60%)]\tLoss: 2.236792\n","Train Epoch: 8 [40000/50000 (80%)]\tLoss: 2.363261\n","\n","Validation set: Average loss: 2.2365, Accuracy: 4046/10000 (40%)\n","\n","Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.936940\n","Train Epoch: 9 [10000/50000 (20%)]\tLoss: 2.217291\n","Train Epoch: 9 [20000/50000 (40%)]\tLoss: 2.019193\n","Train Epoch: 9 [30000/50000 (60%)]\tLoss: 2.329873\n","Train Epoch: 9 [40000/50000 (80%)]\tLoss: 2.244217\n","\n","Validation set: Average loss: 2.0932, Accuracy: 4440/10000 (44%)\n","\n","Train Epoch: 10 [0/50000 (0%)]\tLoss: 2.030207\n","Train Epoch: 10 [10000/50000 (20%)]\tLoss: 2.094553\n","Train Epoch: 10 [20000/50000 (40%)]\tLoss: 1.955013\n","Train Epoch: 10 [30000/50000 (60%)]\tLoss: 2.097170\n","Train Epoch: 10 [40000/50000 (80%)]\tLoss: 2.018227\n","\n","Validation set: Average loss: 2.0680, Accuracy: 4558/10000 (46%)\n","\n","Train Epoch: 11 [0/50000 (0%)]\tLoss: 1.726893\n","Train Epoch: 11 [10000/50000 (20%)]\tLoss: 1.759312\n","Train Epoch: 11 [20000/50000 (40%)]\tLoss: 1.732052\n","Train Epoch: 11 [30000/50000 (60%)]\tLoss: 2.021518\n","Train Epoch: 11 [40000/50000 (80%)]\tLoss: 2.067106\n","\n","Validation set: Average loss: 1.9445, Accuracy: 4801/10000 (48%)\n","\n","Train Epoch: 12 [0/50000 (0%)]\tLoss: 1.780236\n","Train Epoch: 12 [10000/50000 (20%)]\tLoss: 1.908268\n","Train Epoch: 12 [20000/50000 (40%)]\tLoss: 1.827960\n","Train Epoch: 12 [30000/50000 (60%)]\tLoss: 1.818836\n","Train Epoch: 12 [40000/50000 (80%)]\tLoss: 1.843749\n","\n","Validation set: Average loss: 1.8682, Accuracy: 4930/10000 (49%)\n","\n","Train Epoch: 13 [0/50000 (0%)]\tLoss: 1.504857\n","Train Epoch: 13 [10000/50000 (20%)]\tLoss: 1.780118\n","Train Epoch: 13 [20000/50000 (40%)]\tLoss: 1.670255\n","Train Epoch: 13 [30000/50000 (60%)]\tLoss: 1.926128\n","Train Epoch: 13 [40000/50000 (80%)]\tLoss: 1.841184\n","\n","Validation set: Average loss: 1.8482, Accuracy: 5003/10000 (50%)\n","\n","Train Epoch: 14 [0/50000 (0%)]\tLoss: 1.514823\n","Train Epoch: 14 [10000/50000 (20%)]\tLoss: 1.994456\n","Train Epoch: 14 [20000/50000 (40%)]\tLoss: 2.203857\n","Train Epoch: 14 [30000/50000 (60%)]\tLoss: 1.738100\n","Train Epoch: 14 [40000/50000 (80%)]\tLoss: 1.763305\n","\n","Validation set: Average loss: 1.8457, Accuracy: 5139/10000 (51%)\n","\n","Train Epoch: 15 [0/50000 (0%)]\tLoss: 1.694807\n","Train Epoch: 15 [10000/50000 (20%)]\tLoss: 1.289243\n","Train Epoch: 15 [20000/50000 (40%)]\tLoss: 1.363182\n","Train Epoch: 15 [30000/50000 (60%)]\tLoss: 1.517017\n","Train Epoch: 15 [40000/50000 (80%)]\tLoss: 1.744806\n","\n","Validation set: Average loss: 1.7066, Accuracy: 5336/10000 (53%)\n","\n","Train Epoch: 16 [0/50000 (0%)]\tLoss: 1.359149\n","Train Epoch: 16 [10000/50000 (20%)]\tLoss: 1.307850\n","Train Epoch: 16 [20000/50000 (40%)]\tLoss: 1.444950\n","Train Epoch: 16 [30000/50000 (60%)]\tLoss: 1.569832\n","Train Epoch: 16 [40000/50000 (80%)]\tLoss: 1.373091\n","\n","Validation set: Average loss: 1.6944, Accuracy: 5387/10000 (54%)\n","\n","Train Epoch: 17 [0/50000 (0%)]\tLoss: 1.415187\n","Train Epoch: 17 [10000/50000 (20%)]\tLoss: 1.280181\n","Train Epoch: 17 [20000/50000 (40%)]\tLoss: 1.596734\n","Train Epoch: 17 [30000/50000 (60%)]\tLoss: 1.689887\n","Train Epoch: 17 [40000/50000 (80%)]\tLoss: 1.258662\n","\n","Validation set: Average loss: 1.6010, Accuracy: 5639/10000 (56%)\n","\n","Train Epoch: 18 [0/50000 (0%)]\tLoss: 1.353789\n","Train Epoch: 18 [10000/50000 (20%)]\tLoss: 1.371620\n","Train Epoch: 18 [20000/50000 (40%)]\tLoss: 1.747668\n","Train Epoch: 18 [30000/50000 (60%)]\tLoss: 1.477164\n","Train Epoch: 18 [40000/50000 (80%)]\tLoss: 1.670115\n","\n","Validation set: Average loss: 1.6732, Accuracy: 5568/10000 (56%)\n","\n","Train Epoch: 19 [0/50000 (0%)]\tLoss: 1.448836\n","Train Epoch: 19 [10000/50000 (20%)]\tLoss: 1.260280\n","Train Epoch: 19 [20000/50000 (40%)]\tLoss: 1.283673\n","Train Epoch: 19 [30000/50000 (60%)]\tLoss: 1.210278\n","Train Epoch: 19 [40000/50000 (80%)]\tLoss: 1.532509\n","\n","Validation set: Average loss: 1.5652, Accuracy: 5776/10000 (58%)\n","\n","Learning rate updated. New learning rate = 0.055\n","\n","Train Epoch: 20 [0/50000 (0%)]\tLoss: 1.294339\n","Train Epoch: 20 [10000/50000 (20%)]\tLoss: 1.294110\n","Train Epoch: 20 [20000/50000 (40%)]\tLoss: 1.004503\n","Train Epoch: 20 [30000/50000 (60%)]\tLoss: 1.034275\n","Train Epoch: 20 [40000/50000 (80%)]\tLoss: 1.009638\n","\n","Validation set: Average loss: 1.3451, Accuracy: 6305/10000 (63%)\n","\n","Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.931707\n","Train Epoch: 21 [10000/50000 (20%)]\tLoss: 0.761394\n","Train Epoch: 21 [20000/50000 (40%)]\tLoss: 0.847869\n","Train Epoch: 21 [30000/50000 (60%)]\tLoss: 1.197412\n","Train Epoch: 21 [40000/50000 (80%)]\tLoss: 0.990640\n","\n","Validation set: Average loss: 1.3193, Accuracy: 6351/10000 (64%)\n","\n","Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.970433\n","Train Epoch: 22 [10000/50000 (20%)]\tLoss: 1.030079\n","Train Epoch: 22 [20000/50000 (40%)]\tLoss: 1.076409\n","Train Epoch: 22 [30000/50000 (60%)]\tLoss: 1.171423\n","Train Epoch: 22 [40000/50000 (80%)]\tLoss: 0.801766\n","\n","Validation set: Average loss: 1.3459, Accuracy: 6359/10000 (64%)\n","\n","Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.924129\n","Train Epoch: 23 [10000/50000 (20%)]\tLoss: 1.039492\n","Train Epoch: 23 [20000/50000 (40%)]\tLoss: 0.935760\n","Train Epoch: 23 [30000/50000 (60%)]\tLoss: 0.986647\n","Train Epoch: 23 [40000/50000 (80%)]\tLoss: 1.203098\n","\n","Validation set: Average loss: 1.3649, Accuracy: 6321/10000 (63%)\n","\n","Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.937608\n","Train Epoch: 24 [10000/50000 (20%)]\tLoss: 0.805432\n","Train Epoch: 24 [20000/50000 (40%)]\tLoss: 1.050926\n","Train Epoch: 24 [30000/50000 (60%)]\tLoss: 0.967328\n","Train Epoch: 24 [40000/50000 (80%)]\tLoss: 1.261656\n","\n","Validation set: Average loss: 1.3957, Accuracy: 6271/10000 (63%)\n","\n","Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.879506\n","Train Epoch: 25 [10000/50000 (20%)]\tLoss: 1.163018\n","Train Epoch: 25 [20000/50000 (40%)]\tLoss: 0.911511\n","Train Epoch: 25 [30000/50000 (60%)]\tLoss: 1.165808\n","Train Epoch: 25 [40000/50000 (80%)]\tLoss: 0.879512\n","\n","Validation set: Average loss: 1.3735, Accuracy: 6348/10000 (63%)\n","\n","Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.833262\n","Train Epoch: 26 [10000/50000 (20%)]\tLoss: 0.640305\n","Train Epoch: 26 [20000/50000 (40%)]\tLoss: 0.925618\n","Train Epoch: 26 [30000/50000 (60%)]\tLoss: 1.297406\n","Train Epoch: 26 [40000/50000 (80%)]\tLoss: 0.933150\n","\n","Validation set: Average loss: 1.3889, Accuracy: 6295/10000 (63%)\n","\n","Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.774702\n","Train Epoch: 27 [10000/50000 (20%)]\tLoss: 0.753097\n","Train Epoch: 27 [20000/50000 (40%)]\tLoss: 0.800834\n","Train Epoch: 27 [30000/50000 (60%)]\tLoss: 0.716860\n","Train Epoch: 27 [40000/50000 (80%)]\tLoss: 1.018829\n","\n","Validation set: Average loss: 1.3593, Accuracy: 6414/10000 (64%)\n","\n","Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.727152\n","Train Epoch: 28 [10000/50000 (20%)]\tLoss: 0.787215\n","Train Epoch: 28 [20000/50000 (40%)]\tLoss: 0.842122\n","Train Epoch: 28 [30000/50000 (60%)]\tLoss: 0.728325\n","Train Epoch: 28 [40000/50000 (80%)]\tLoss: 0.742292\n","\n","Validation set: Average loss: 1.3789, Accuracy: 6330/10000 (63%)\n","\n","Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.929323\n","Train Epoch: 29 [10000/50000 (20%)]\tLoss: 0.876731\n","Train Epoch: 29 [20000/50000 (40%)]\tLoss: 0.984578\n","Train Epoch: 29 [30000/50000 (60%)]\tLoss: 0.854855\n","Train Epoch: 29 [40000/50000 (80%)]\tLoss: 0.832438\n","\n","Validation set: Average loss: 1.4571, Accuracy: 6235/10000 (62%)\n","\n","Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.519505\n","Train Epoch: 30 [10000/50000 (20%)]\tLoss: 0.608542\n","Train Epoch: 30 [20000/50000 (40%)]\tLoss: 0.824073\n","Train Epoch: 30 [30000/50000 (60%)]\tLoss: 0.930531\n","Train Epoch: 30 [40000/50000 (80%)]\tLoss: 0.959796\n","\n","Validation set: Average loss: 1.4425, Accuracy: 6193/10000 (62%)\n","\n","Train Epoch: 31 [0/50000 (0%)]\tLoss: 0.752532\n","Train Epoch: 31 [10000/50000 (20%)]\tLoss: 0.983894\n","Train Epoch: 31 [20000/50000 (40%)]\tLoss: 0.948940\n","Train Epoch: 31 [30000/50000 (60%)]\tLoss: 0.712921\n","Train Epoch: 31 [40000/50000 (80%)]\tLoss: 0.977106\n","\n","Validation set: Average loss: 1.3952, Accuracy: 6332/10000 (63%)\n","\n","Train Epoch: 32 [0/50000 (0%)]\tLoss: 0.717700\n","Train Epoch: 32 [10000/50000 (20%)]\tLoss: 0.880453\n","Train Epoch: 32 [20000/50000 (40%)]\tLoss: 0.719188\n","Train Epoch: 32 [30000/50000 (60%)]\tLoss: 0.729941\n","Train Epoch: 32 [40000/50000 (80%)]\tLoss: 0.734304\n","\n","Validation set: Average loss: 1.4018, Accuracy: 6396/10000 (64%)\n","\n","Train Epoch: 33 [0/50000 (0%)]\tLoss: 0.656969\n","Train Epoch: 33 [10000/50000 (20%)]\tLoss: 0.815452\n","Train Epoch: 33 [20000/50000 (40%)]\tLoss: 0.881668\n","Train Epoch: 33 [30000/50000 (60%)]\tLoss: 1.033790\n","Train Epoch: 33 [40000/50000 (80%)]\tLoss: 0.827437\n","\n","Validation set: Average loss: 1.3554, Accuracy: 6513/10000 (65%)\n","\n","Train Epoch: 34 [0/50000 (0%)]\tLoss: 0.747721\n","Train Epoch: 34 [10000/50000 (20%)]\tLoss: 0.616594\n","Train Epoch: 34 [20000/50000 (40%)]\tLoss: 0.711380\n","Train Epoch: 34 [30000/50000 (60%)]\tLoss: 0.689884\n","Train Epoch: 34 [40000/50000 (80%)]\tLoss: 0.840583\n","\n","Validation set: Average loss: 1.3595, Accuracy: 6464/10000 (65%)\n","\n","Train Epoch: 35 [0/50000 (0%)]\tLoss: 0.694354\n","Train Epoch: 35 [10000/50000 (20%)]\tLoss: 0.627209\n","Train Epoch: 35 [20000/50000 (40%)]\tLoss: 0.790454\n","Train Epoch: 35 [30000/50000 (60%)]\tLoss: 0.475023\n","Train Epoch: 35 [40000/50000 (80%)]\tLoss: 0.761144\n","\n","Validation set: Average loss: 1.4552, Accuracy: 6314/10000 (63%)\n","\n","Train Epoch: 36 [0/50000 (0%)]\tLoss: 0.621601\n","Train Epoch: 36 [10000/50000 (20%)]\tLoss: 0.590949\n","Train Epoch: 36 [20000/50000 (40%)]\tLoss: 0.674835\n","Train Epoch: 36 [30000/50000 (60%)]\tLoss: 0.765089\n","Train Epoch: 36 [40000/50000 (80%)]\tLoss: 0.669886\n","\n","Validation set: Average loss: 1.3918, Accuracy: 6466/10000 (65%)\n","\n","Train Epoch: 37 [0/50000 (0%)]\tLoss: 0.653625\n","Train Epoch: 37 [10000/50000 (20%)]\tLoss: 0.516350\n","Train Epoch: 37 [20000/50000 (40%)]\tLoss: 0.697520\n","Train Epoch: 37 [30000/50000 (60%)]\tLoss: 0.823437\n","Train Epoch: 37 [40000/50000 (80%)]\tLoss: 0.677016\n","\n","Validation set: Average loss: 1.4675, Accuracy: 6353/10000 (64%)\n","\n","Train Epoch: 38 [0/50000 (0%)]\tLoss: 0.436618\n","Train Epoch: 38 [10000/50000 (20%)]\tLoss: 0.558870\n","Train Epoch: 38 [20000/50000 (40%)]\tLoss: 0.892734\n","Train Epoch: 38 [30000/50000 (60%)]\tLoss: 0.644872\n","Train Epoch: 38 [40000/50000 (80%)]\tLoss: 0.686934\n","\n","Validation set: Average loss: 1.4193, Accuracy: 6369/10000 (64%)\n","\n","Train Epoch: 39 [0/50000 (0%)]\tLoss: 0.718642\n","Train Epoch: 39 [10000/50000 (20%)]\tLoss: 0.569916\n","Train Epoch: 39 [20000/50000 (40%)]\tLoss: 0.598250\n","Train Epoch: 39 [30000/50000 (60%)]\tLoss: 0.686124\n","Train Epoch: 39 [40000/50000 (80%)]\tLoss: 0.693463\n","\n","Validation set: Average loss: 1.4335, Accuracy: 6457/10000 (65%)\n","\n","Learning rate updated. New learning rate = 0.0275\n","\n","Train Epoch: 40 [0/50000 (0%)]\tLoss: 0.409272\n","Train Epoch: 40 [10000/50000 (20%)]\tLoss: 0.401119\n","Train Epoch: 40 [20000/50000 (40%)]\tLoss: 0.322222\n","Train Epoch: 40 [30000/50000 (60%)]\tLoss: 0.310366\n","Train Epoch: 40 [40000/50000 (80%)]\tLoss: 0.396746\n","\n","Validation set: Average loss: 1.1707, Accuracy: 6999/10000 (70%)\n","\n","Train Epoch: 41 [0/50000 (0%)]\tLoss: 0.344496\n","Train Epoch: 41 [10000/50000 (20%)]\tLoss: 0.165779\n","Train Epoch: 41 [20000/50000 (40%)]\tLoss: 0.402722\n","Train Epoch: 41 [30000/50000 (60%)]\tLoss: 0.399142\n","Train Epoch: 41 [40000/50000 (80%)]\tLoss: 0.399206\n","\n","Validation set: Average loss: 1.2175, Accuracy: 6942/10000 (69%)\n","\n","Train Epoch: 42 [0/50000 (0%)]\tLoss: 0.274166\n","Train Epoch: 42 [10000/50000 (20%)]\tLoss: 0.375021\n","Train Epoch: 42 [20000/50000 (40%)]\tLoss: 0.380099\n","Train Epoch: 42 [30000/50000 (60%)]\tLoss: 0.251544\n","Train Epoch: 42 [40000/50000 (80%)]\tLoss: 0.378084\n","\n","Validation set: Average loss: 1.2124, Accuracy: 6963/10000 (70%)\n","\n","Train Epoch: 43 [0/50000 (0%)]\tLoss: 0.247153\n","Train Epoch: 43 [10000/50000 (20%)]\tLoss: 0.368124\n","Train Epoch: 43 [20000/50000 (40%)]\tLoss: 0.317825\n","Train Epoch: 43 [30000/50000 (60%)]\tLoss: 0.220785\n","Train Epoch: 43 [40000/50000 (80%)]\tLoss: 0.305893\n","\n","Validation set: Average loss: 1.2375, Accuracy: 6953/10000 (70%)\n","\n","Train Epoch: 44 [0/50000 (0%)]\tLoss: 0.256490\n","Train Epoch: 44 [10000/50000 (20%)]\tLoss: 0.206762\n","Train Epoch: 44 [20000/50000 (40%)]\tLoss: 0.333685\n","Train Epoch: 44 [30000/50000 (60%)]\tLoss: 0.249992\n","Train Epoch: 44 [40000/50000 (80%)]\tLoss: 0.324759\n","\n","Validation set: Average loss: 1.2493, Accuracy: 6963/10000 (70%)\n","\n","Train Epoch: 45 [0/50000 (0%)]\tLoss: 0.245096\n","Train Epoch: 45 [10000/50000 (20%)]\tLoss: 0.267450\n","Train Epoch: 45 [20000/50000 (40%)]\tLoss: 0.359637\n","Train Epoch: 45 [30000/50000 (60%)]\tLoss: 0.306395\n","Train Epoch: 45 [40000/50000 (80%)]\tLoss: 0.237692\n","\n","Validation set: Average loss: 1.3001, Accuracy: 6853/10000 (69%)\n","\n","Train Epoch: 46 [0/50000 (0%)]\tLoss: 0.297451\n","Train Epoch: 46 [10000/50000 (20%)]\tLoss: 0.222329\n","Train Epoch: 46 [20000/50000 (40%)]\tLoss: 0.322354\n","Train Epoch: 46 [30000/50000 (60%)]\tLoss: 0.203084\n","Train Epoch: 46 [40000/50000 (80%)]\tLoss: 0.239170\n","\n","Validation set: Average loss: 1.3229, Accuracy: 6795/10000 (68%)\n","\n","Train Epoch: 47 [0/50000 (0%)]\tLoss: 0.262534\n","Train Epoch: 47 [10000/50000 (20%)]\tLoss: 0.170589\n","Train Epoch: 47 [20000/50000 (40%)]\tLoss: 0.289740\n","Train Epoch: 47 [30000/50000 (60%)]\tLoss: 0.254344\n","Train Epoch: 47 [40000/50000 (80%)]\tLoss: 0.255155\n","\n","Validation set: Average loss: 1.3248, Accuracy: 6808/10000 (68%)\n","\n","Train Epoch: 48 [0/50000 (0%)]\tLoss: 0.196689\n","Train Epoch: 48 [10000/50000 (20%)]\tLoss: 0.337964\n","Train Epoch: 48 [20000/50000 (40%)]\tLoss: 0.238241\n","Train Epoch: 48 [30000/50000 (60%)]\tLoss: 0.268246\n","Train Epoch: 48 [40000/50000 (80%)]\tLoss: 0.241119\n","\n","Validation set: Average loss: 1.2937, Accuracy: 6902/10000 (69%)\n","\n","Train Epoch: 49 [0/50000 (0%)]\tLoss: 0.266230\n","Train Epoch: 49 [10000/50000 (20%)]\tLoss: 0.229971\n","Train Epoch: 49 [20000/50000 (40%)]\tLoss: 0.260150\n","Train Epoch: 49 [30000/50000 (60%)]\tLoss: 0.248612\n","Train Epoch: 49 [40000/50000 (80%)]\tLoss: 0.384374\n","\n","Validation set: Average loss: 1.3545, Accuracy: 6838/10000 (68%)\n","\n","Train Epoch: 50 [0/50000 (0%)]\tLoss: 0.196150\n","Train Epoch: 50 [10000/50000 (20%)]\tLoss: 0.261530\n","Train Epoch: 50 [20000/50000 (40%)]\tLoss: 0.225271\n","Train Epoch: 50 [30000/50000 (60%)]\tLoss: 0.345864\n","Train Epoch: 50 [40000/50000 (80%)]\tLoss: 0.360690\n","\n","Validation set: Average loss: 1.3529, Accuracy: 6831/10000 (68%)\n","\n","Train Epoch: 51 [0/50000 (0%)]\tLoss: 0.239677\n","Train Epoch: 51 [10000/50000 (20%)]\tLoss: 0.224609\n","Train Epoch: 51 [20000/50000 (40%)]\tLoss: 0.230987\n","Train Epoch: 51 [30000/50000 (60%)]\tLoss: 0.172006\n","Train Epoch: 51 [40000/50000 (80%)]\tLoss: 0.256348\n","\n","Validation set: Average loss: 1.4038, Accuracy: 6786/10000 (68%)\n","\n","Train Epoch: 52 [0/50000 (0%)]\tLoss: 0.240843\n","Train Epoch: 52 [10000/50000 (20%)]\tLoss: 0.295688\n","Train Epoch: 52 [20000/50000 (40%)]\tLoss: 0.255654\n","Train Epoch: 52 [30000/50000 (60%)]\tLoss: 0.164920\n","Train Epoch: 52 [40000/50000 (80%)]\tLoss: 0.205852\n","\n","Validation set: Average loss: 1.4111, Accuracy: 6790/10000 (68%)\n","\n","Train Epoch: 53 [0/50000 (0%)]\tLoss: 0.153871\n","Train Epoch: 53 [10000/50000 (20%)]\tLoss: 0.197269\n","Train Epoch: 53 [20000/50000 (40%)]\tLoss: 0.157775\n","Train Epoch: 53 [30000/50000 (60%)]\tLoss: 0.197829\n","Train Epoch: 53 [40000/50000 (80%)]\tLoss: 0.398498\n","\n","Validation set: Average loss: 1.3721, Accuracy: 6863/10000 (69%)\n","\n","Train Epoch: 54 [0/50000 (0%)]\tLoss: 0.293143\n","Train Epoch: 54 [10000/50000 (20%)]\tLoss: 0.198506\n","Train Epoch: 54 [20000/50000 (40%)]\tLoss: 0.338125\n","Train Epoch: 54 [30000/50000 (60%)]\tLoss: 0.278249\n","Train Epoch: 54 [40000/50000 (80%)]\tLoss: 0.354563\n","\n","Validation set: Average loss: 1.4123, Accuracy: 6740/10000 (67%)\n","\n","Train Epoch: 55 [0/50000 (0%)]\tLoss: 0.221940\n","Train Epoch: 55 [10000/50000 (20%)]\tLoss: 0.215195\n","Train Epoch: 55 [20000/50000 (40%)]\tLoss: 0.283152\n","Train Epoch: 55 [30000/50000 (60%)]\tLoss: 0.356686\n","Train Epoch: 55 [40000/50000 (80%)]\tLoss: 0.326747\n","\n","Validation set: Average loss: 1.3864, Accuracy: 6785/10000 (68%)\n","\n","Train Epoch: 56 [0/50000 (0%)]\tLoss: 0.184424\n","Train Epoch: 56 [10000/50000 (20%)]\tLoss: 0.158656\n","Train Epoch: 56 [20000/50000 (40%)]\tLoss: 0.243715\n","Train Epoch: 56 [30000/50000 (60%)]\tLoss: 0.323612\n","Train Epoch: 56 [40000/50000 (80%)]\tLoss: 0.272329\n","\n","Validation set: Average loss: 1.4208, Accuracy: 6757/10000 (68%)\n","\n","Train Epoch: 57 [0/50000 (0%)]\tLoss: 0.199669\n","Train Epoch: 57 [10000/50000 (20%)]\tLoss: 0.220789\n","Train Epoch: 57 [20000/50000 (40%)]\tLoss: 0.236456\n","Train Epoch: 57 [30000/50000 (60%)]\tLoss: 0.196266\n","Train Epoch: 57 [40000/50000 (80%)]\tLoss: 0.245885\n","\n","Validation set: Average loss: 1.4274, Accuracy: 6753/10000 (68%)\n","\n","Train Epoch: 58 [0/50000 (0%)]\tLoss: 0.208880\n","Train Epoch: 58 [10000/50000 (20%)]\tLoss: 0.192027\n","Train Epoch: 58 [20000/50000 (40%)]\tLoss: 0.298742\n","Train Epoch: 58 [30000/50000 (60%)]\tLoss: 0.194460\n","Train Epoch: 58 [40000/50000 (80%)]\tLoss: 0.283550\n","\n","Validation set: Average loss: 1.4226, Accuracy: 6778/10000 (68%)\n","\n","Train Epoch: 59 [0/50000 (0%)]\tLoss: 0.157490\n","Train Epoch: 59 [10000/50000 (20%)]\tLoss: 0.222659\n","Train Epoch: 59 [20000/50000 (40%)]\tLoss: 0.251036\n","Train Epoch: 59 [30000/50000 (60%)]\tLoss: 0.255247\n","Train Epoch: 59 [40000/50000 (80%)]\tLoss: 0.271456\n","\n","Validation set: Average loss: 1.4272, Accuracy: 6747/10000 (67%)\n","\n","Learning rate updated. New learning rate = 0.01375\n","\n","Train Epoch: 60 [0/50000 (0%)]\tLoss: 0.147612\n","Train Epoch: 60 [10000/50000 (20%)]\tLoss: 0.122388\n","Train Epoch: 60 [20000/50000 (40%)]\tLoss: 0.087391\n","Train Epoch: 60 [30000/50000 (60%)]\tLoss: 0.100617\n","Train Epoch: 60 [40000/50000 (80%)]\tLoss: 0.104863\n","\n","Validation set: Average loss: 1.2472, Accuracy: 7086/10000 (71%)\n","\n","Train Epoch: 61 [0/50000 (0%)]\tLoss: 0.096440\n","Train Epoch: 61 [10000/50000 (20%)]\tLoss: 0.032169\n","Train Epoch: 61 [20000/50000 (40%)]\tLoss: 0.085601\n","Train Epoch: 61 [30000/50000 (60%)]\tLoss: 0.185456\n","Train Epoch: 61 [40000/50000 (80%)]\tLoss: 0.104209\n","\n","Validation set: Average loss: 1.2444, Accuracy: 7136/10000 (71%)\n","\n","Train Epoch: 62 [0/50000 (0%)]\tLoss: 0.059959\n","Train Epoch: 62 [10000/50000 (20%)]\tLoss: 0.075496\n","Train Epoch: 62 [20000/50000 (40%)]\tLoss: 0.054825\n","Train Epoch: 62 [30000/50000 (60%)]\tLoss: 0.091968\n","Train Epoch: 62 [40000/50000 (80%)]\tLoss: 0.099099\n","\n","Validation set: Average loss: 1.2399, Accuracy: 7149/10000 (71%)\n","\n","Train Epoch: 63 [0/50000 (0%)]\tLoss: 0.058775\n","Train Epoch: 63 [10000/50000 (20%)]\tLoss: 0.064517\n","Train Epoch: 63 [20000/50000 (40%)]\tLoss: 0.073795\n","Train Epoch: 63 [30000/50000 (60%)]\tLoss: 0.039745\n","Train Epoch: 63 [40000/50000 (80%)]\tLoss: 0.050371\n","\n","Validation set: Average loss: 1.2651, Accuracy: 7150/10000 (72%)\n","\n","Train Epoch: 64 [0/50000 (0%)]\tLoss: 0.044938\n","Train Epoch: 64 [10000/50000 (20%)]\tLoss: 0.058327\n","Train Epoch: 64 [20000/50000 (40%)]\tLoss: 0.044125\n","Train Epoch: 64 [30000/50000 (60%)]\tLoss: 0.063985\n","Train Epoch: 64 [40000/50000 (80%)]\tLoss: 0.048038\n","\n","Validation set: Average loss: 1.2520, Accuracy: 7189/10000 (72%)\n","\n","Train Epoch: 65 [0/50000 (0%)]\tLoss: 0.042159\n","Train Epoch: 65 [10000/50000 (20%)]\tLoss: 0.075903\n","Train Epoch: 65 [20000/50000 (40%)]\tLoss: 0.089687\n","Train Epoch: 65 [30000/50000 (60%)]\tLoss: 0.068812\n","Train Epoch: 65 [40000/50000 (80%)]\tLoss: 0.050155\n","\n","Validation set: Average loss: 1.2937, Accuracy: 7076/10000 (71%)\n","\n","Train Epoch: 66 [0/50000 (0%)]\tLoss: 0.078824\n","Train Epoch: 66 [10000/50000 (20%)]\tLoss: 0.089057\n","Train Epoch: 66 [20000/50000 (40%)]\tLoss: 0.058279\n","Train Epoch: 66 [30000/50000 (60%)]\tLoss: 0.040487\n","Train Epoch: 66 [40000/50000 (80%)]\tLoss: 0.045976\n","\n","Validation set: Average loss: 1.2659, Accuracy: 7153/10000 (72%)\n","\n","Train Epoch: 67 [0/50000 (0%)]\tLoss: 0.035821\n","Train Epoch: 67 [10000/50000 (20%)]\tLoss: 0.101428\n","Train Epoch: 67 [20000/50000 (40%)]\tLoss: 0.059357\n","Train Epoch: 67 [30000/50000 (60%)]\tLoss: 0.116493\n","Train Epoch: 67 [40000/50000 (80%)]\tLoss: 0.027948\n","\n","Validation set: Average loss: 1.2678, Accuracy: 7160/10000 (72%)\n","\n","Train Epoch: 68 [0/50000 (0%)]\tLoss: 0.048130\n","Train Epoch: 68 [10000/50000 (20%)]\tLoss: 0.053876\n","Train Epoch: 68 [20000/50000 (40%)]\tLoss: 0.046524\n","Train Epoch: 68 [30000/50000 (60%)]\tLoss: 0.108140\n","Train Epoch: 68 [40000/50000 (80%)]\tLoss: 0.044215\n","\n","Validation set: Average loss: 1.2625, Accuracy: 7167/10000 (72%)\n","\n","Train Epoch: 69 [0/50000 (0%)]\tLoss: 0.032760\n","Train Epoch: 69 [10000/50000 (20%)]\tLoss: 0.060973\n","Train Epoch: 69 [20000/50000 (40%)]\tLoss: 0.045258\n","Train Epoch: 69 [30000/50000 (60%)]\tLoss: 0.051735\n","Train Epoch: 69 [40000/50000 (80%)]\tLoss: 0.036519\n","\n","Validation set: Average loss: 1.2755, Accuracy: 7127/10000 (71%)\n","\n","Train Epoch: 70 [0/50000 (0%)]\tLoss: 0.066818\n","Train Epoch: 70 [10000/50000 (20%)]\tLoss: 0.048489\n","Train Epoch: 70 [20000/50000 (40%)]\tLoss: 0.058847\n","Train Epoch: 70 [30000/50000 (60%)]\tLoss: 0.090488\n","Train Epoch: 70 [40000/50000 (80%)]\tLoss: 0.072641\n","\n","Validation set: Average loss: 1.2676, Accuracy: 7137/10000 (71%)\n","\n","Train Epoch: 71 [0/50000 (0%)]\tLoss: 0.084279\n","Train Epoch: 71 [10000/50000 (20%)]\tLoss: 0.051722\n","Train Epoch: 71 [20000/50000 (40%)]\tLoss: 0.046311\n","Train Epoch: 71 [30000/50000 (60%)]\tLoss: 0.059531\n","Train Epoch: 71 [40000/50000 (80%)]\tLoss: 0.078977\n","\n","Validation set: Average loss: 1.3081, Accuracy: 7113/10000 (71%)\n","\n","Train Epoch: 72 [0/50000 (0%)]\tLoss: 0.024331\n","Train Epoch: 72 [10000/50000 (20%)]\tLoss: 0.050922\n","Train Epoch: 72 [20000/50000 (40%)]\tLoss: 0.060456\n","Train Epoch: 72 [30000/50000 (60%)]\tLoss: 0.042957\n","Train Epoch: 72 [40000/50000 (80%)]\tLoss: 0.057864\n","\n","Validation set: Average loss: 1.2794, Accuracy: 7104/10000 (71%)\n","\n","Train Epoch: 73 [0/50000 (0%)]\tLoss: 0.036786\n","Train Epoch: 73 [10000/50000 (20%)]\tLoss: 0.051839\n","Train Epoch: 73 [20000/50000 (40%)]\tLoss: 0.085640\n","Train Epoch: 73 [30000/50000 (60%)]\tLoss: 0.051584\n","Train Epoch: 73 [40000/50000 (80%)]\tLoss: 0.078626\n","\n","Validation set: Average loss: 1.2906, Accuracy: 7082/10000 (71%)\n","\n","Train Epoch: 74 [0/50000 (0%)]\tLoss: 0.065303\n","Train Epoch: 74 [10000/50000 (20%)]\tLoss: 0.119095\n","Train Epoch: 74 [20000/50000 (40%)]\tLoss: 0.056427\n","Train Epoch: 74 [30000/50000 (60%)]\tLoss: 0.028900\n","Train Epoch: 74 [40000/50000 (80%)]\tLoss: 0.038713\n","\n","Validation set: Average loss: 1.2907, Accuracy: 7109/10000 (71%)\n","\n","Train Epoch: 75 [0/50000 (0%)]\tLoss: 0.077019\n","Train Epoch: 75 [10000/50000 (20%)]\tLoss: 0.029132\n","Train Epoch: 75 [20000/50000 (40%)]\tLoss: 0.030586\n","Train Epoch: 75 [30000/50000 (60%)]\tLoss: 0.016019\n","Train Epoch: 75 [40000/50000 (80%)]\tLoss: 0.025660\n","\n","Validation set: Average loss: 1.2981, Accuracy: 7105/10000 (71%)\n","\n","Train Epoch: 76 [0/50000 (0%)]\tLoss: 0.029392\n","Train Epoch: 76 [10000/50000 (20%)]\tLoss: 0.031294\n","Train Epoch: 76 [20000/50000 (40%)]\tLoss: 0.046728\n","Train Epoch: 76 [30000/50000 (60%)]\tLoss: 0.050198\n","Train Epoch: 76 [40000/50000 (80%)]\tLoss: 0.067276\n","\n","Validation set: Average loss: 1.2760, Accuracy: 7135/10000 (71%)\n","\n","Train Epoch: 77 [0/50000 (0%)]\tLoss: 0.048112\n","Train Epoch: 77 [10000/50000 (20%)]\tLoss: 0.018334\n","Train Epoch: 77 [20000/50000 (40%)]\tLoss: 0.057211\n","Train Epoch: 77 [30000/50000 (60%)]\tLoss: 0.038242\n","Train Epoch: 77 [40000/50000 (80%)]\tLoss: 0.050660\n","\n","Validation set: Average loss: 1.2645, Accuracy: 7171/10000 (72%)\n","\n","Train Epoch: 78 [0/50000 (0%)]\tLoss: 0.013639\n","Train Epoch: 78 [10000/50000 (20%)]\tLoss: 0.026856\n","Train Epoch: 78 [20000/50000 (40%)]\tLoss: 0.060403\n","Train Epoch: 78 [30000/50000 (60%)]\tLoss: 0.045935\n","Train Epoch: 78 [40000/50000 (80%)]\tLoss: 0.069288\n","\n","Validation set: Average loss: 1.2790, Accuracy: 7114/10000 (71%)\n","\n","Train Epoch: 79 [0/50000 (0%)]\tLoss: 0.048793\n","Train Epoch: 79 [10000/50000 (20%)]\tLoss: 0.064529\n","Train Epoch: 79 [20000/50000 (40%)]\tLoss: 0.048874\n","Train Epoch: 79 [30000/50000 (60%)]\tLoss: 0.101979\n","Train Epoch: 79 [40000/50000 (80%)]\tLoss: 0.081252\n","\n","Validation set: Average loss: 1.2836, Accuracy: 7120/10000 (71%)\n","\n","Learning rate updated. New learning rate = 0.006875\n","\n","Train Epoch: 80 [0/50000 (0%)]\tLoss: 0.049712\n","Train Epoch: 80 [10000/50000 (20%)]\tLoss: 0.024692\n","Train Epoch: 80 [20000/50000 (40%)]\tLoss: 0.025319\n","Train Epoch: 80 [30000/50000 (60%)]\tLoss: 0.054004\n","Train Epoch: 80 [40000/50000 (80%)]\tLoss: 0.037358\n","\n","Validation set: Average loss: 1.2441, Accuracy: 7220/10000 (72%)\n","\n","Train Epoch: 81 [0/50000 (0%)]\tLoss: 0.024410\n","Train Epoch: 81 [10000/50000 (20%)]\tLoss: 0.029636\n","Train Epoch: 81 [20000/50000 (40%)]\tLoss: 0.048293\n","Train Epoch: 81 [30000/50000 (60%)]\tLoss: 0.022236\n","Train Epoch: 81 [40000/50000 (80%)]\tLoss: 0.020955\n","\n","Validation set: Average loss: 1.2288, Accuracy: 7204/10000 (72%)\n","\n","Train Epoch: 82 [0/50000 (0%)]\tLoss: 0.006640\n","Train Epoch: 82 [10000/50000 (20%)]\tLoss: 0.024407\n","Train Epoch: 82 [20000/50000 (40%)]\tLoss: 0.008179\n","Train Epoch: 82 [30000/50000 (60%)]\tLoss: 0.041113\n","Train Epoch: 82 [40000/50000 (80%)]\tLoss: 0.045179\n","\n","Validation set: Average loss: 1.2406, Accuracy: 7224/10000 (72%)\n","\n","Train Epoch: 83 [0/50000 (0%)]\tLoss: 0.023889\n","Train Epoch: 83 [10000/50000 (20%)]\tLoss: 0.035660\n","Train Epoch: 83 [20000/50000 (40%)]\tLoss: 0.025685\n","Train Epoch: 83 [30000/50000 (60%)]\tLoss: 0.020068\n","Train Epoch: 83 [40000/50000 (80%)]\tLoss: 0.022246\n","\n","Validation set: Average loss: 1.2207, Accuracy: 7230/10000 (72%)\n","\n","Train Epoch: 84 [0/50000 (0%)]\tLoss: 0.023499\n","Train Epoch: 84 [10000/50000 (20%)]\tLoss: 0.054295\n","Train Epoch: 84 [20000/50000 (40%)]\tLoss: 0.023257\n","Train Epoch: 84 [30000/50000 (60%)]\tLoss: 0.046581\n","Train Epoch: 84 [40000/50000 (80%)]\tLoss: 0.026217\n","\n","Validation set: Average loss: 1.2184, Accuracy: 7245/10000 (72%)\n","\n","Train Epoch: 85 [0/50000 (0%)]\tLoss: 0.027442\n","Train Epoch: 85 [10000/50000 (20%)]\tLoss: 0.034724\n","Train Epoch: 85 [20000/50000 (40%)]\tLoss: 0.018036\n","Train Epoch: 85 [30000/50000 (60%)]\tLoss: 0.015979\n","Train Epoch: 85 [40000/50000 (80%)]\tLoss: 0.016324\n","\n","Validation set: Average loss: 1.2344, Accuracy: 7209/10000 (72%)\n","\n","Train Epoch: 86 [0/50000 (0%)]\tLoss: 0.031926\n","Train Epoch: 86 [10000/50000 (20%)]\tLoss: 0.030968\n","Train Epoch: 86 [20000/50000 (40%)]\tLoss: 0.015314\n","Train Epoch: 86 [30000/50000 (60%)]\tLoss: 0.012734\n","Train Epoch: 86 [40000/50000 (80%)]\tLoss: 0.012137\n","\n","Validation set: Average loss: 1.2284, Accuracy: 7222/10000 (72%)\n","\n","Train Epoch: 87 [0/50000 (0%)]\tLoss: 0.020430\n","Train Epoch: 87 [10000/50000 (20%)]\tLoss: 0.016586\n","Train Epoch: 87 [20000/50000 (40%)]\tLoss: 0.022645\n","Train Epoch: 87 [30000/50000 (60%)]\tLoss: 0.035883\n","Train Epoch: 87 [40000/50000 (80%)]\tLoss: 0.062421\n","\n","Validation set: Average loss: 1.2204, Accuracy: 7235/10000 (72%)\n","\n","Train Epoch: 88 [0/50000 (0%)]\tLoss: 0.018214\n","Train Epoch: 88 [10000/50000 (20%)]\tLoss: 0.017387\n","Train Epoch: 88 [20000/50000 (40%)]\tLoss: 0.017737\n","Train Epoch: 88 [30000/50000 (60%)]\tLoss: 0.048070\n","Train Epoch: 88 [40000/50000 (80%)]\tLoss: 0.019132\n","\n","Validation set: Average loss: 1.2279, Accuracy: 7250/10000 (72%)\n","\n","Train Epoch: 89 [0/50000 (0%)]\tLoss: 0.005734\n","Train Epoch: 89 [10000/50000 (20%)]\tLoss: 0.009478\n","Train Epoch: 89 [20000/50000 (40%)]\tLoss: 0.016301\n","Train Epoch: 89 [30000/50000 (60%)]\tLoss: 0.046786\n","Train Epoch: 89 [40000/50000 (80%)]\tLoss: 0.023559\n","\n","Validation set: Average loss: 1.2132, Accuracy: 7271/10000 (73%)\n","\n","Train Epoch: 90 [0/50000 (0%)]\tLoss: 0.022556\n","Train Epoch: 90 [10000/50000 (20%)]\tLoss: 0.021667\n","Train Epoch: 90 [20000/50000 (40%)]\tLoss: 0.015021\n","Train Epoch: 90 [30000/50000 (60%)]\tLoss: 0.009973\n","Train Epoch: 90 [40000/50000 (80%)]\tLoss: 0.009604\n","\n","Validation set: Average loss: 1.2092, Accuracy: 7275/10000 (73%)\n","\n","Train Epoch: 91 [0/50000 (0%)]\tLoss: 0.010951\n","Train Epoch: 91 [10000/50000 (20%)]\tLoss: 0.018597\n","Train Epoch: 91 [20000/50000 (40%)]\tLoss: 0.011270\n","Train Epoch: 91 [30000/50000 (60%)]\tLoss: 0.007526\n","Train Epoch: 91 [40000/50000 (80%)]\tLoss: 0.020199\n","\n","Validation set: Average loss: 1.2097, Accuracy: 7275/10000 (73%)\n","\n","Train Epoch: 92 [0/50000 (0%)]\tLoss: 0.005565\n","Train Epoch: 92 [10000/50000 (20%)]\tLoss: 0.017884\n","Train Epoch: 92 [20000/50000 (40%)]\tLoss: 0.011425\n","Train Epoch: 92 [30000/50000 (60%)]\tLoss: 0.009858\n","Train Epoch: 92 [40000/50000 (80%)]\tLoss: 0.016471\n","\n","Validation set: Average loss: 1.2207, Accuracy: 7249/10000 (72%)\n","\n","Train Epoch: 93 [0/50000 (0%)]\tLoss: 0.016605\n","Train Epoch: 93 [10000/50000 (20%)]\tLoss: 0.029294\n","Train Epoch: 93 [20000/50000 (40%)]\tLoss: 0.008852\n","Train Epoch: 93 [30000/50000 (60%)]\tLoss: 0.033804\n","Train Epoch: 93 [40000/50000 (80%)]\tLoss: 0.012877\n","\n","Validation set: Average loss: 1.2108, Accuracy: 7255/10000 (73%)\n","\n","Train Epoch: 94 [0/50000 (0%)]\tLoss: 0.012184\n","Train Epoch: 94 [10000/50000 (20%)]\tLoss: 0.022399\n","Train Epoch: 94 [20000/50000 (40%)]\tLoss: 0.031332\n","Train Epoch: 94 [30000/50000 (60%)]\tLoss: 0.018622\n","Train Epoch: 94 [40000/50000 (80%)]\tLoss: 0.030827\n","\n","Validation set: Average loss: 1.2050, Accuracy: 7263/10000 (73%)\n","\n","Train Epoch: 95 [0/50000 (0%)]\tLoss: 0.012137\n","Train Epoch: 95 [10000/50000 (20%)]\tLoss: 0.012518\n","Train Epoch: 95 [20000/50000 (40%)]\tLoss: 0.010214\n","Train Epoch: 95 [30000/50000 (60%)]\tLoss: 0.048269\n","Train Epoch: 95 [40000/50000 (80%)]\tLoss: 0.039469\n","\n","Validation set: Average loss: 1.2172, Accuracy: 7237/10000 (72%)\n","\n","Train Epoch: 96 [0/50000 (0%)]\tLoss: 0.018991\n","Train Epoch: 96 [10000/50000 (20%)]\tLoss: 0.027074\n","Train Epoch: 96 [20000/50000 (40%)]\tLoss: 0.037582\n","Train Epoch: 96 [30000/50000 (60%)]\tLoss: 0.018131\n","Train Epoch: 96 [40000/50000 (80%)]\tLoss: 0.027186\n","\n","Validation set: Average loss: 1.2137, Accuracy: 7271/10000 (73%)\n","\n","Train Epoch: 97 [0/50000 (0%)]\tLoss: 0.014735\n","Train Epoch: 97 [10000/50000 (20%)]\tLoss: 0.018035\n","Train Epoch: 97 [20000/50000 (40%)]\tLoss: 0.015338\n","Train Epoch: 97 [30000/50000 (60%)]\tLoss: 0.017868\n","Train Epoch: 97 [40000/50000 (80%)]\tLoss: 0.011157\n","\n","Validation set: Average loss: 1.2087, Accuracy: 7262/10000 (73%)\n","\n","Train Epoch: 98 [0/50000 (0%)]\tLoss: 0.014165\n","Train Epoch: 98 [10000/50000 (20%)]\tLoss: 0.014138\n","Train Epoch: 98 [20000/50000 (40%)]\tLoss: 0.027656\n","Train Epoch: 98 [30000/50000 (60%)]\tLoss: 0.015121\n","Train Epoch: 98 [40000/50000 (80%)]\tLoss: 0.014109\n","\n","Validation set: Average loss: 1.2134, Accuracy: 7272/10000 (73%)\n","\n","Train Epoch: 99 [0/50000 (0%)]\tLoss: 0.011089\n","Train Epoch: 99 [10000/50000 (20%)]\tLoss: 0.014244\n","Train Epoch: 99 [20000/50000 (40%)]\tLoss: 0.009581\n","Train Epoch: 99 [30000/50000 (60%)]\tLoss: 0.011721\n","Train Epoch: 99 [40000/50000 (80%)]\tLoss: 0.009900\n","\n","Validation set: Average loss: 1.2010, Accuracy: 7274/10000 (73%)\n","\n","Learning rate updated. New learning rate = 0.0034375\n","\n","Train Epoch: 100 [0/50000 (0%)]\tLoss: 0.011608\n","Train Epoch: 100 [10000/50000 (20%)]\tLoss: 0.012886\n","Train Epoch: 100 [20000/50000 (40%)]\tLoss: 0.015849\n","Train Epoch: 100 [30000/50000 (60%)]\tLoss: 0.020338\n","Train Epoch: 100 [40000/50000 (80%)]\tLoss: 0.031190\n","\n","Validation set: Average loss: 1.1957, Accuracy: 7308/10000 (73%)\n","\n","Train Epoch: 101 [0/50000 (0%)]\tLoss: 0.016300\n","Train Epoch: 101 [10000/50000 (20%)]\tLoss: 0.007138\n","Train Epoch: 101 [20000/50000 (40%)]\tLoss: 0.021988\n","Train Epoch: 101 [30000/50000 (60%)]\tLoss: 0.014234\n","Train Epoch: 101 [40000/50000 (80%)]\tLoss: 0.014828\n","\n","Validation set: Average loss: 1.1909, Accuracy: 7316/10000 (73%)\n","\n","Train Epoch: 102 [0/50000 (0%)]\tLoss: 0.013294\n","Train Epoch: 102 [10000/50000 (20%)]\tLoss: 0.022065\n","Train Epoch: 102 [20000/50000 (40%)]\tLoss: 0.017111\n","Train Epoch: 102 [30000/50000 (60%)]\tLoss: 0.018405\n","Train Epoch: 102 [40000/50000 (80%)]\tLoss: 0.054852\n","\n","Validation set: Average loss: 1.1959, Accuracy: 7298/10000 (73%)\n","\n","Train Epoch: 103 [0/50000 (0%)]\tLoss: 0.016660\n","Train Epoch: 103 [10000/50000 (20%)]\tLoss: 0.007349\n","Train Epoch: 103 [20000/50000 (40%)]\tLoss: 0.011223\n","Train Epoch: 103 [30000/50000 (60%)]\tLoss: 0.006277\n","Train Epoch: 103 [40000/50000 (80%)]\tLoss: 0.010772\n","\n","Validation set: Average loss: 1.1876, Accuracy: 7326/10000 (73%)\n","\n","Train Epoch: 104 [0/50000 (0%)]\tLoss: 0.009620\n","Train Epoch: 104 [10000/50000 (20%)]\tLoss: 0.009791\n","Train Epoch: 104 [20000/50000 (40%)]\tLoss: 0.011893\n","Train Epoch: 104 [30000/50000 (60%)]\tLoss: 0.013248\n","Train Epoch: 104 [40000/50000 (80%)]\tLoss: 0.022893\n","\n","Validation set: Average loss: 1.1890, Accuracy: 7328/10000 (73%)\n","\n","Train Epoch: 105 [0/50000 (0%)]\tLoss: 0.022310\n","Train Epoch: 105 [10000/50000 (20%)]\tLoss: 0.026879\n","Train Epoch: 105 [20000/50000 (40%)]\tLoss: 0.007890\n","Train Epoch: 105 [30000/50000 (60%)]\tLoss: 0.019299\n","Train Epoch: 105 [40000/50000 (80%)]\tLoss: 0.011224\n","\n","Validation set: Average loss: 1.1821, Accuracy: 7312/10000 (73%)\n","\n","Train Epoch: 106 [0/50000 (0%)]\tLoss: 0.018024\n","Train Epoch: 106 [10000/50000 (20%)]\tLoss: 0.006355\n","Train Epoch: 106 [20000/50000 (40%)]\tLoss: 0.009558\n","Train Epoch: 106 [30000/50000 (60%)]\tLoss: 0.009949\n","Train Epoch: 106 [40000/50000 (80%)]\tLoss: 0.015974\n","\n","Validation set: Average loss: 1.1875, Accuracy: 7297/10000 (73%)\n","\n","Train Epoch: 107 [0/50000 (0%)]\tLoss: 0.006740\n","Train Epoch: 107 [10000/50000 (20%)]\tLoss: 0.024098\n","Train Epoch: 107 [20000/50000 (40%)]\tLoss: 0.008604\n","Train Epoch: 107 [30000/50000 (60%)]\tLoss: 0.009546\n","Train Epoch: 107 [40000/50000 (80%)]\tLoss: 0.012470\n","\n","Validation set: Average loss: 1.1854, Accuracy: 7290/10000 (73%)\n","\n","Train Epoch: 108 [0/50000 (0%)]\tLoss: 0.013050\n","Train Epoch: 108 [10000/50000 (20%)]\tLoss: 0.019493\n","Train Epoch: 108 [20000/50000 (40%)]\tLoss: 0.010599\n","Train Epoch: 108 [30000/50000 (60%)]\tLoss: 0.015269\n","Train Epoch: 108 [40000/50000 (80%)]\tLoss: 0.009073\n","\n","Validation set: Average loss: 1.1919, Accuracy: 7289/10000 (73%)\n","\n","Train Epoch: 109 [0/50000 (0%)]\tLoss: 0.006979\n","Train Epoch: 109 [10000/50000 (20%)]\tLoss: 0.007737\n","Train Epoch: 109 [20000/50000 (40%)]\tLoss: 0.011809\n","Train Epoch: 109 [30000/50000 (60%)]\tLoss: 0.008165\n","Train Epoch: 109 [40000/50000 (80%)]\tLoss: 0.014322\n","\n","Validation set: Average loss: 1.1836, Accuracy: 7301/10000 (73%)\n","\n","Train Epoch: 110 [0/50000 (0%)]\tLoss: 0.011490\n","Train Epoch: 110 [10000/50000 (20%)]\tLoss: 0.024898\n","Train Epoch: 110 [20000/50000 (40%)]\tLoss: 0.003722\n","Train Epoch: 110 [30000/50000 (60%)]\tLoss: 0.005983\n","Train Epoch: 110 [40000/50000 (80%)]\tLoss: 0.006975\n","\n","Validation set: Average loss: 1.1897, Accuracy: 7289/10000 (73%)\n","\n","Train Epoch: 111 [0/50000 (0%)]\tLoss: 0.008664\n","Train Epoch: 111 [10000/50000 (20%)]\tLoss: 0.014070\n","Train Epoch: 111 [20000/50000 (40%)]\tLoss: 0.013153\n","Train Epoch: 111 [30000/50000 (60%)]\tLoss: 0.004722\n","Train Epoch: 111 [40000/50000 (80%)]\tLoss: 0.011225\n","\n","Validation set: Average loss: 1.1849, Accuracy: 7293/10000 (73%)\n","\n","Train Epoch: 112 [0/50000 (0%)]\tLoss: 0.008278\n","Train Epoch: 112 [10000/50000 (20%)]\tLoss: 0.008877\n","Train Epoch: 112 [20000/50000 (40%)]\tLoss: 0.018526\n","Train Epoch: 112 [30000/50000 (60%)]\tLoss: 0.037041\n","Train Epoch: 112 [40000/50000 (80%)]\tLoss: 0.017459\n","\n","Validation set: Average loss: 1.1855, Accuracy: 7289/10000 (73%)\n","\n","Train Epoch: 113 [0/50000 (0%)]\tLoss: 0.018087\n","Train Epoch: 113 [10000/50000 (20%)]\tLoss: 0.021706\n","Train Epoch: 113 [20000/50000 (40%)]\tLoss: 0.008482\n","Train Epoch: 113 [30000/50000 (60%)]\tLoss: 0.029576\n","Train Epoch: 113 [40000/50000 (80%)]\tLoss: 0.010137\n","\n","Validation set: Average loss: 1.1828, Accuracy: 7298/10000 (73%)\n","\n","Train Epoch: 114 [0/50000 (0%)]\tLoss: 0.021619\n","Train Epoch: 114 [10000/50000 (20%)]\tLoss: 0.008058\n","Train Epoch: 114 [20000/50000 (40%)]\tLoss: 0.004951\n","Train Epoch: 114 [30000/50000 (60%)]\tLoss: 0.006597\n","Train Epoch: 114 [40000/50000 (80%)]\tLoss: 0.009697\n","\n","Validation set: Average loss: 1.1798, Accuracy: 7291/10000 (73%)\n","\n","Train Epoch: 115 [0/50000 (0%)]\tLoss: 0.008861\n","Train Epoch: 115 [10000/50000 (20%)]\tLoss: 0.016046\n","Train Epoch: 115 [20000/50000 (40%)]\tLoss: 0.012343\n","Train Epoch: 115 [30000/50000 (60%)]\tLoss: 0.010983\n","Train Epoch: 115 [40000/50000 (80%)]\tLoss: 0.016519\n","\n","Validation set: Average loss: 1.1766, Accuracy: 7301/10000 (73%)\n","\n","Train Epoch: 116 [0/50000 (0%)]\tLoss: 0.009754\n","Train Epoch: 116 [10000/50000 (20%)]\tLoss: 0.006201\n","Train Epoch: 116 [20000/50000 (40%)]\tLoss: 0.018984\n","Train Epoch: 116 [30000/50000 (60%)]\tLoss: 0.007593\n","Train Epoch: 116 [40000/50000 (80%)]\tLoss: 0.010110\n","\n","Validation set: Average loss: 1.1739, Accuracy: 7289/10000 (73%)\n","\n","Train Epoch: 117 [0/50000 (0%)]\tLoss: 0.007582\n","Train Epoch: 117 [10000/50000 (20%)]\tLoss: 0.009575\n","Train Epoch: 117 [20000/50000 (40%)]\tLoss: 0.009927\n","Train Epoch: 117 [30000/50000 (60%)]\tLoss: 0.040903\n","Train Epoch: 117 [40000/50000 (80%)]\tLoss: 0.010992\n","\n","Validation set: Average loss: 1.1731, Accuracy: 7305/10000 (73%)\n","\n","Train Epoch: 118 [0/50000 (0%)]\tLoss: 0.008292\n","Train Epoch: 118 [10000/50000 (20%)]\tLoss: 0.013949\n","Train Epoch: 118 [20000/50000 (40%)]\tLoss: 0.011854\n","Train Epoch: 118 [30000/50000 (60%)]\tLoss: 0.013193\n","Train Epoch: 118 [40000/50000 (80%)]\tLoss: 0.019030\n","\n","Validation set: Average loss: 1.1755, Accuracy: 7275/10000 (73%)\n","\n","Train Epoch: 119 [0/50000 (0%)]\tLoss: 0.013645\n","Train Epoch: 119 [10000/50000 (20%)]\tLoss: 0.012653\n","Train Epoch: 119 [20000/50000 (40%)]\tLoss: 0.022251\n","Train Epoch: 119 [30000/50000 (60%)]\tLoss: 0.007430\n","Train Epoch: 119 [40000/50000 (80%)]\tLoss: 0.011005\n","\n","Validation set: Average loss: 1.1822, Accuracy: 7268/10000 (73%)\n","\n","Learning rate updated. New learning rate = 0.00171875\n","\n","Train Epoch: 120 [0/50000 (0%)]\tLoss: 0.007440\n","Train Epoch: 120 [10000/50000 (20%)]\tLoss: 0.013896\n","Train Epoch: 120 [20000/50000 (40%)]\tLoss: 0.010048\n","Train Epoch: 120 [30000/50000 (60%)]\tLoss: 0.013615\n","Train Epoch: 120 [40000/50000 (80%)]\tLoss: 0.024691\n","\n","Validation set: Average loss: 1.1767, Accuracy: 7274/10000 (73%)\n","\n","Train Epoch: 121 [0/50000 (0%)]\tLoss: 0.012337\n","Train Epoch: 121 [10000/50000 (20%)]\tLoss: 0.031099\n","Train Epoch: 121 [20000/50000 (40%)]\tLoss: 0.013131\n","Train Epoch: 121 [30000/50000 (60%)]\tLoss: 0.010321\n","Train Epoch: 121 [40000/50000 (80%)]\tLoss: 0.007071\n","\n","Validation set: Average loss: 1.1771, Accuracy: 7286/10000 (73%)\n","\n","Train Epoch: 122 [0/50000 (0%)]\tLoss: 0.008292\n","Train Epoch: 122 [10000/50000 (20%)]\tLoss: 0.009259\n","Train Epoch: 122 [20000/50000 (40%)]\tLoss: 0.013450\n","Train Epoch: 122 [30000/50000 (60%)]\tLoss: 0.025110\n","Train Epoch: 122 [40000/50000 (80%)]\tLoss: 0.025284\n","\n","Validation set: Average loss: 1.1716, Accuracy: 7289/10000 (73%)\n","\n","Train Epoch: 123 [0/50000 (0%)]\tLoss: 0.007948\n","Train Epoch: 123 [10000/50000 (20%)]\tLoss: 0.009186\n","Train Epoch: 123 [20000/50000 (40%)]\tLoss: 0.003643\n","Train Epoch: 123 [30000/50000 (60%)]\tLoss: 0.012793\n","Train Epoch: 123 [40000/50000 (80%)]\tLoss: 0.005499\n","\n","Validation set: Average loss: 1.1724, Accuracy: 7319/10000 (73%)\n","\n","Train Epoch: 124 [0/50000 (0%)]\tLoss: 0.010915\n","Train Epoch: 124 [10000/50000 (20%)]\tLoss: 0.009734\n","Train Epoch: 124 [20000/50000 (40%)]\tLoss: 0.006385\n","Train Epoch: 124 [30000/50000 (60%)]\tLoss: 0.016738\n","Train Epoch: 124 [40000/50000 (80%)]\tLoss: 0.019223\n","\n","Validation set: Average loss: 1.1705, Accuracy: 7316/10000 (73%)\n","\n","Train Epoch: 125 [0/50000 (0%)]\tLoss: 0.010827\n","Train Epoch: 125 [10000/50000 (20%)]\tLoss: 0.028155\n","Train Epoch: 125 [20000/50000 (40%)]\tLoss: 0.014658\n","Train Epoch: 125 [30000/50000 (60%)]\tLoss: 0.006709\n","Train Epoch: 125 [40000/50000 (80%)]\tLoss: 0.006909\n","\n","Validation set: Average loss: 1.1694, Accuracy: 7308/10000 (73%)\n","\n","Train Epoch: 126 [0/50000 (0%)]\tLoss: 0.013119\n","Train Epoch: 126 [10000/50000 (20%)]\tLoss: 0.012179\n","Train Epoch: 126 [20000/50000 (40%)]\tLoss: 0.005713\n","Train Epoch: 126 [30000/50000 (60%)]\tLoss: 0.014795\n","Train Epoch: 126 [40000/50000 (80%)]\tLoss: 0.019403\n","\n","Validation set: Average loss: 1.1712, Accuracy: 7317/10000 (73%)\n","\n","Train Epoch: 127 [0/50000 (0%)]\tLoss: 0.013969\n","Train Epoch: 127 [10000/50000 (20%)]\tLoss: 0.007317\n","Train Epoch: 127 [20000/50000 (40%)]\tLoss: 0.007769\n","Train Epoch: 127 [30000/50000 (60%)]\tLoss: 0.010129\n","Train Epoch: 127 [40000/50000 (80%)]\tLoss: 0.022289\n","\n","Validation set: Average loss: 1.1712, Accuracy: 7319/10000 (73%)\n","\n","Train Epoch: 128 [0/50000 (0%)]\tLoss: 0.006998\n","Train Epoch: 128 [10000/50000 (20%)]\tLoss: 0.023136\n","Train Epoch: 128 [20000/50000 (40%)]\tLoss: 0.024465\n","Train Epoch: 128 [30000/50000 (60%)]\tLoss: 0.018560\n","Train Epoch: 128 [40000/50000 (80%)]\tLoss: 0.008263\n","\n","Validation set: Average loss: 1.1712, Accuracy: 7309/10000 (73%)\n","\n","Train Epoch: 129 [0/50000 (0%)]\tLoss: 0.007946\n","Train Epoch: 129 [10000/50000 (20%)]\tLoss: 0.009188\n","Train Epoch: 129 [20000/50000 (40%)]\tLoss: 0.008405\n","Train Epoch: 129 [30000/50000 (60%)]\tLoss: 0.005359\n","Train Epoch: 129 [40000/50000 (80%)]\tLoss: 0.007843\n","\n","Validation set: Average loss: 1.1718, Accuracy: 7320/10000 (73%)\n","\n","Train Epoch: 130 [0/50000 (0%)]\tLoss: 0.007342\n","Train Epoch: 130 [10000/50000 (20%)]\tLoss: 0.008425\n","Train Epoch: 130 [20000/50000 (40%)]\tLoss: 0.006740\n","Train Epoch: 130 [30000/50000 (60%)]\tLoss: 0.014697\n","Train Epoch: 130 [40000/50000 (80%)]\tLoss: 0.010407\n","\n","Validation set: Average loss: 1.1737, Accuracy: 7293/10000 (73%)\n","\n","Train Epoch: 131 [0/50000 (0%)]\tLoss: 0.006444\n","Train Epoch: 131 [10000/50000 (20%)]\tLoss: 0.015671\n","Train Epoch: 131 [20000/50000 (40%)]\tLoss: 0.005094\n","Train Epoch: 131 [30000/50000 (60%)]\tLoss: 0.010512\n","Train Epoch: 131 [40000/50000 (80%)]\tLoss: 0.005952\n","\n","Validation set: Average loss: 1.1703, Accuracy: 7289/10000 (73%)\n","\n","Train Epoch: 132 [0/50000 (0%)]\tLoss: 0.011526\n","Train Epoch: 132 [10000/50000 (20%)]\tLoss: 0.008422\n","Train Epoch: 132 [20000/50000 (40%)]\tLoss: 0.006055\n","Train Epoch: 132 [30000/50000 (60%)]\tLoss: 0.012093\n","Train Epoch: 132 [40000/50000 (80%)]\tLoss: 0.010335\n","\n","Validation set: Average loss: 1.1713, Accuracy: 7323/10000 (73%)\n","\n","Train Epoch: 133 [0/50000 (0%)]\tLoss: 0.020558\n","Train Epoch: 133 [10000/50000 (20%)]\tLoss: 0.004431\n","Train Epoch: 133 [20000/50000 (40%)]\tLoss: 0.007332\n","Train Epoch: 133 [30000/50000 (60%)]\tLoss: 0.010990\n","Train Epoch: 133 [40000/50000 (80%)]\tLoss: 0.009528\n","\n","Validation set: Average loss: 1.1706, Accuracy: 7290/10000 (73%)\n","\n","Train Epoch: 134 [0/50000 (0%)]\tLoss: 0.006105\n","Train Epoch: 134 [10000/50000 (20%)]\tLoss: 0.011937\n","Train Epoch: 134 [20000/50000 (40%)]\tLoss: 0.019958\n","Train Epoch: 134 [30000/50000 (60%)]\tLoss: 0.010199\n","Train Epoch: 134 [40000/50000 (80%)]\tLoss: 0.016837\n","\n","Validation set: Average loss: 1.1685, Accuracy: 7298/10000 (73%)\n","\n","Train Epoch: 135 [0/50000 (0%)]\tLoss: 0.008670\n","Train Epoch: 135 [10000/50000 (20%)]\tLoss: 0.005937\n","Train Epoch: 135 [20000/50000 (40%)]\tLoss: 0.009522\n","Train Epoch: 135 [30000/50000 (60%)]\tLoss: 0.003458\n","Train Epoch: 135 [40000/50000 (80%)]\tLoss: 0.010237\n","\n","Validation set: Average loss: 1.1656, Accuracy: 7305/10000 (73%)\n","\n","Train Epoch: 136 [0/50000 (0%)]\tLoss: 0.013539\n","Train Epoch: 136 [10000/50000 (20%)]\tLoss: 0.007944\n","Train Epoch: 136 [20000/50000 (40%)]\tLoss: 0.005549\n","Train Epoch: 136 [30000/50000 (60%)]\tLoss: 0.007116\n","Train Epoch: 136 [40000/50000 (80%)]\tLoss: 0.017894\n","\n","Validation set: Average loss: 1.1684, Accuracy: 7316/10000 (73%)\n","\n","Train Epoch: 137 [0/50000 (0%)]\tLoss: 0.007863\n","Train Epoch: 137 [10000/50000 (20%)]\tLoss: 0.006984\n","Train Epoch: 137 [20000/50000 (40%)]\tLoss: 0.004765\n","Train Epoch: 137 [30000/50000 (60%)]\tLoss: 0.005629\n","Train Epoch: 137 [40000/50000 (80%)]\tLoss: 0.009864\n","\n","Validation set: Average loss: 1.1690, Accuracy: 7309/10000 (73%)\n","\n","Train Epoch: 138 [0/50000 (0%)]\tLoss: 0.005913\n","Train Epoch: 138 [10000/50000 (20%)]\tLoss: 0.008860\n","Train Epoch: 138 [20000/50000 (40%)]\tLoss: 0.004559\n","Train Epoch: 138 [30000/50000 (60%)]\tLoss: 0.006429\n","Train Epoch: 138 [40000/50000 (80%)]\tLoss: 0.012680\n","\n","Validation set: Average loss: 1.1707, Accuracy: 7312/10000 (73%)\n","\n","Train Epoch: 139 [0/50000 (0%)]\tLoss: 0.005593\n","Train Epoch: 139 [10000/50000 (20%)]\tLoss: 0.007319\n","Train Epoch: 139 [20000/50000 (40%)]\tLoss: 0.005145\n","Train Epoch: 139 [30000/50000 (60%)]\tLoss: 0.010370\n","Train Epoch: 139 [40000/50000 (80%)]\tLoss: 0.005374\n","\n","Validation set: Average loss: 1.1658, Accuracy: 7301/10000 (73%)\n","\n","Learning rate updated. New learning rate = 0.000859375\n","\n","Train Epoch: 140 [0/50000 (0%)]\tLoss: 0.007345\n","Train Epoch: 140 [10000/50000 (20%)]\tLoss: 0.011945\n","Train Epoch: 140 [20000/50000 (40%)]\tLoss: 0.009113\n","Train Epoch: 140 [30000/50000 (60%)]\tLoss: 0.010829\n","Train Epoch: 140 [40000/50000 (80%)]\tLoss: 0.012734\n","\n","Validation set: Average loss: 1.1631, Accuracy: 7297/10000 (73%)\n","\n","Train Epoch: 141 [0/50000 (0%)]\tLoss: 0.010403\n","Train Epoch: 141 [10000/50000 (20%)]\tLoss: 0.009985\n","Train Epoch: 141 [20000/50000 (40%)]\tLoss: 0.004647\n","Train Epoch: 141 [30000/50000 (60%)]\tLoss: 0.008396\n","Train Epoch: 141 [40000/50000 (80%)]\tLoss: 0.021802\n","\n","Validation set: Average loss: 1.1586, Accuracy: 7322/10000 (73%)\n","\n","Train Epoch: 142 [0/50000 (0%)]\tLoss: 0.005343\n","Train Epoch: 142 [10000/50000 (20%)]\tLoss: 0.011469\n","Train Epoch: 142 [20000/50000 (40%)]\tLoss: 0.005821\n","Train Epoch: 142 [30000/50000 (60%)]\tLoss: 0.007437\n","Train Epoch: 142 [40000/50000 (80%)]\tLoss: 0.009964\n","\n","Validation set: Average loss: 1.1657, Accuracy: 7296/10000 (73%)\n","\n","Train Epoch: 143 [0/50000 (0%)]\tLoss: 0.007207\n","Train Epoch: 143 [10000/50000 (20%)]\tLoss: 0.010473\n","Train Epoch: 143 [20000/50000 (40%)]\tLoss: 0.010813\n","Train Epoch: 143 [30000/50000 (60%)]\tLoss: 0.012396\n","Train Epoch: 143 [40000/50000 (80%)]\tLoss: 0.003339\n","\n","Validation set: Average loss: 1.1674, Accuracy: 7307/10000 (73%)\n","\n","Train Epoch: 144 [0/50000 (0%)]\tLoss: 0.020959\n","Train Epoch: 144 [10000/50000 (20%)]\tLoss: 0.005158\n","Train Epoch: 144 [20000/50000 (40%)]\tLoss: 0.010252\n","Train Epoch: 144 [30000/50000 (60%)]\tLoss: 0.004516\n","Train Epoch: 144 [40000/50000 (80%)]\tLoss: 0.008426\n","\n","Validation set: Average loss: 1.1623, Accuracy: 7316/10000 (73%)\n","\n","Train Epoch: 145 [0/50000 (0%)]\tLoss: 0.009765\n","Train Epoch: 145 [10000/50000 (20%)]\tLoss: 0.012342\n","Train Epoch: 145 [20000/50000 (40%)]\tLoss: 0.006691\n","Train Epoch: 145 [30000/50000 (60%)]\tLoss: 0.011608\n","Train Epoch: 145 [40000/50000 (80%)]\tLoss: 0.012698\n","\n","Validation set: Average loss: 1.1663, Accuracy: 7311/10000 (73%)\n","\n","Train Epoch: 146 [0/50000 (0%)]\tLoss: 0.036523\n","Train Epoch: 146 [10000/50000 (20%)]\tLoss: 0.007332\n","Train Epoch: 146 [20000/50000 (40%)]\tLoss: 0.021209\n","Train Epoch: 146 [30000/50000 (60%)]\tLoss: 0.017594\n","Train Epoch: 146 [40000/50000 (80%)]\tLoss: 0.013542\n","\n","Validation set: Average loss: 1.1625, Accuracy: 7317/10000 (73%)\n","\n","Train Epoch: 147 [0/50000 (0%)]\tLoss: 0.020722\n","Train Epoch: 147 [10000/50000 (20%)]\tLoss: 0.010626\n","Train Epoch: 147 [20000/50000 (40%)]\tLoss: 0.020573\n","Train Epoch: 147 [30000/50000 (60%)]\tLoss: 0.005575\n","Train Epoch: 147 [40000/50000 (80%)]\tLoss: 0.015951\n","\n","Validation set: Average loss: 1.1649, Accuracy: 7309/10000 (73%)\n","\n","Train Epoch: 148 [0/50000 (0%)]\tLoss: 0.006658\n","Train Epoch: 148 [10000/50000 (20%)]\tLoss: 0.009861\n","Train Epoch: 148 [20000/50000 (40%)]\tLoss: 0.005225\n","Train Epoch: 148 [30000/50000 (60%)]\tLoss: 0.007621\n","Train Epoch: 148 [40000/50000 (80%)]\tLoss: 0.007852\n","\n","Validation set: Average loss: 1.1649, Accuracy: 7316/10000 (73%)\n","\n","Train Epoch: 149 [0/50000 (0%)]\tLoss: 0.005371\n","Train Epoch: 149 [10000/50000 (20%)]\tLoss: 0.007396\n","Train Epoch: 149 [20000/50000 (40%)]\tLoss: 0.009606\n","Train Epoch: 149 [30000/50000 (60%)]\tLoss: 0.009762\n","Train Epoch: 149 [40000/50000 (80%)]\tLoss: 0.021061\n","\n","Validation set: Average loss: 1.1615, Accuracy: 7332/10000 (73%)\n","\n","Train Epoch: 150 [0/50000 (0%)]\tLoss: 0.012336\n","Train Epoch: 150 [10000/50000 (20%)]\tLoss: 0.007031\n","Train Epoch: 150 [20000/50000 (40%)]\tLoss: 0.015347\n","Train Epoch: 150 [30000/50000 (60%)]\tLoss: 0.008727\n","Train Epoch: 150 [40000/50000 (80%)]\tLoss: 0.011386\n","\n","Validation set: Average loss: 1.1615, Accuracy: 7321/10000 (73%)\n","\n","CPU times: user 2h 43min 32s, sys: 1h 15min 3s, total: 3h 58min 35s\n","Wall time: 3h 58min 38s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J6gU7Gqioc-C","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(5,3))\n","plt.plot(np.arange(1,num_epochs+1), lossv)\n","plt.title('validation loss')\n","\n","plt.figure(figsize=(5,3))\n","plt.plot(np.arange(1,num_epochs+1), accv)\n","plt.title('validation accuracy');"],"execution_count":0,"outputs":[]}]}